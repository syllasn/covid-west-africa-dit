{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning  Long Short-Term Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/11/keras-long-short-term-memory-lstm-model-predict-stock-prices.html\n",
    "https://www.kdnuggets.com/2018/11/sales-forecasting-using-prophet.html\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    training_set = df_senegal_prophet.iloc[:, 1:2].values\n",
    "    df_senegal_prophet.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    #sc = MinMaxScaler(feature_range = (0, 1))\n",
    "    #sc\n",
    "    training_set_scaled = fit_transform(df_senegal_prophet)\n",
    "   \n",
    "    range(60, 2035)\n",
    "   \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(6, 20):\n",
    "        print(df_senegal_prophet[i-6:i, 0])\n",
    "        #X_train.append(df_senegal_prophet[i-6:i, 0])\n",
    "        y_train.append(df_senegal_prophet[i, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "   \n",
    "    # Load the statsmodels api\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Load your dataset\n",
    "    #endog = df_senegal_prophet['']\n",
    "    endog=df_senegal[['Daily_case']]\n",
    "    \n",
    "    # We could fit an AR(2) model, described above\n",
    "    mod_ar2 = sm.tsa.SARIMAX(endog, order=(2,0,0))\n",
    "    # Note that mod_ar2 is an instance of the SARIMAX class\\n\n",
    "   \n",
    "\n",
    "\n",
    "    # Fit the model via maximum likelihood\n",
    "    res_ar2 = mod_ar2.fit()\n",
    "    # Note that res_ar2 is an instance of the SARIMAXResults class\n",
    "    \n",
    "    # Show the summary of results\n",
    "    print(res_ar2.summary())\n",
    "    \n",
    "    # We could also fit a more complicated model with seasonal components.\n",
    "    # As an example, here is an SARIMA(1,1,1) x (0,1,1,4):\n",
    "    mod_sarimax = sm.tsa.SARIMAX(endog, order=(1,1,1),\n",
    "                                 seasonal_order=(0,1,1,4))\n",
    "    res_sarimax = mod_sarimax.fit()\n",
    "    \n",
    "    # Show the summary of results\n",
    "    print(res_sarimax.summary())\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Load the statsmodels api\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Load your dataset\n",
    "    #endog = pd.read_csv('your/dataset/here.csv')\n",
    "    \n",
    "    # Fit a local level model\n",
    "    mod_ll = sm.tsa.UnobservedComponents(endog, 'local level')\n",
    "    # Note that mod_ll is an instance of the UnobservedComponents class\n",
    "    \n",
    "    # Fit the model via maximum likelihood\n",
    "    res_ll = mod_ll.fit()\n",
    "    # Note that res_ll is an instance of the UnobservedComponentsResults class\n",
    "    \n",
    "    # Show the summary of results\n",
    "    print(res_ll.summary())\n",
    "    \n",
    "    # Show a plot of the estimated level and trend component series\n",
    "    fig_ll = res_ll.plot_components()\n",
    "    \n",
    "    # We could further add a damped stochastic cycle as follows\n",
    "    mod_cycle = sm.tsa.UnobservedComponents(endog, 'local level', cycle=True,\n",
    "                                            damped_cycle=true,\n",
    "                                            stochastic_cycle=True)\n",
    "    res_cycle = mod_cycle.fit()\n",
    "    \n",
    "    # Show the summary of results\n",
    "    print(res_cycle.summary())\n",
    "    \\n\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Show a plot of the estimated level, trend, and cycle component series\n",
    "    fig_cycle = res_cycle.plot_components()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Load the statsmodels api\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Load your (multivariate) dataset\n",
    "    #endog = pd.read_csv('your/dataset/here.csv')\n",
    "    \n",
    "    # Fit a local level model\n",
    "    mod_var1 = sm.tsa.VARMAX(endog, order=(1,0))\n",
    "    # Note that mod_var1 is an instance of the VARMAX class\n",
    "    \n",
    "    # Fit the model via maximum likelihood\n",
    "    res_var1 = mod_var1.fit()\n",
    "    # Note that res_var1 is an instance of the VARMAXResults class\n",
    "    \n",
    "    # Show the summary of results\n",
    "    print(res_var1.summary())\n",
    "    \n",
    "    # Construct impulse responses\n",
    "    irfs = res_ll.impulse_responses(steps=10)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: markdown,\n",
    "   metadata: {},\n",
    "   source: [\n",
    "    # Time Series Model \n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    \n",
    "    #X = confirmed_cases\n",
    "    X =df_senegal[['Daily_case']]\n",
    "    size = int(len(X) * 0.66)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    plt.figure(figsize=(15,5))\n",
    "    pyplot.plot(X)\n",
    "    pyplot.xticks(rotation = 90)\n",
    "    pyplot.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    #Y = confirmed_cases.values\n",
    "    Y = X.values\n",
    "    \n",
    "    \n",
    "    #Y = confirmed_cases\n",
    "    \n",
    "    \n",
    "    train_size = int(len(Y) * 0.66)\n",
    "    train, test = Y[0:train_size], Y[train_size:len(Y)]\n",
    "    print('Observations: %d' % (len(Y)))\n",
    "    print('Training Observations: %d' % (len(train)))\n",
    "    print('Testing Observations: %d' % (len(test)))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    plt.figure(figsize=(15,5))\n",
    "    pyplot.plot(train)\n",
    "    pyplot.xticks(rotation = 90)\n",
    "    pyplot.plot([None for i in train] + [x for x in test])\n",
    "    pyplot.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    plt.figure(figsize=(15,5))\n",
    "    autocorrelation_plot(X)\n",
    "    pyplot.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # fit model\n",
    "    model = ARIMA(X, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    print(model_fit.summary())\n",
    "    \\n\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # plot residual errors\n",
    "    residuals = DataFrame(model_fit.resid)\n",
    "    residuals.plot()\n",
    "    pyplot.show()\n",
    "    residuals.plot(kind='kde')\n",
    "    pyplot.show()\n",
    "    print(residuals.describe())\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    #Xvalues = confirmed_cases.values\n",
    "    Xvalues = X.values\n",
    "    \n",
    "    \n",
    "    size = int(len(Xvalues) * 0.66)\n",
    "    train, test = Xvalues[0:size], Xvalues[size:len(Xvalues)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "    \\tmodel = ARIMA(history, order=(3,1,0))\n",
    "    \\tmodel_fit = model.fit(disp=0)\n",
    "    \\toutput = model_fit.forecast()\n",
    "    \\tyhat = output[0]\n",
    "    \\tpredictions.append(yhat)\n",
    "    \\tobs = test[t]\n",
    "    \\thistory.append(obs)\n",
    "    \\tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    \\n\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    print('Test MSE: %.3f' % error)\n",
    "    # plot\n",
    "    pyplot.plot(test)\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    \\tdataset = dataset.astype('float32')\n",
    "    \\tbest_score, best_cfg = float(\\inf\\), None\n",
    "    \\tfor p in p_values:\n",
    "    \\t\\tfor d in d_values:\n",
    "    \\t\\t\\tfor q in q_values:\n",
    "    \\t\\t\\t\\torder = (p,d,q)\n",
    "    \\t\\t\\t\\ttry:\n",
    "    \\t\\t\\t\\t\\tmse = evaluate_arima_model(dataset, order)\n",
    "    \\t\\t\\t\\t\\tif mse < best_score:\n",
    "    \\t\\t\\t\\t\\t\\tbest_score, best_cfg = mse, order\n",
    "    \\t\\t\\t\\t\\tprint('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "    \\t\\t\\t\\texcept:\n",
    "    \\t\\t\\t\\t\\tcontinue\n",
    "    \\tprint('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\\ignore\\)\n",
    "    # evaluate parameters\n",
    "    p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "    d_values = range(0, 3)\n",
    "    q_values = range(0, 3)\n",
    "    warnings.filterwarnings(\\ignore\\)\n",
    "    evaluate_models(X.values, p_values, d_values, q_values)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # evaluate an ARIMA model for a given order (p,d,q)\n",
    "    def evaluate_arima_model(X, arima_order):\n",
    "    \\t# prepare training dataset\n",
    "    \\ttrain_size = int(len(X) * 0.66)\n",
    "    \\ttrain, test = X[0:train_size], X[train_size:]\n",
    "    \\thistory = [x for x in train]\n",
    "    \\t# make predictions\n",
    "    \\tpredictions = list()\n",
    "    \\tfor t in range(len(test)):\n",
    "    \\t\\tmodel = ARIMA(history, order=arima_order)\n",
    "    \\t\\tmodel_fit = model.fit(disp=0)\n",
    "    \\t\\tyhat = model_fit.forecast()[0]\n",
    "    \\t\\tpredictions.append(yhat)\n",
    "    \\t\\thistory.append(test[t])\n",
    "    \\t# calculate out of sample error\n",
    "    \\terror = mean_squared_error(test, predictions)\n",
    "    \\treturn error\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # load dataset\n",
    "    \n",
    "    \n",
    "    # evaluate parameters\n",
    "    p_values = [0, 1, 2, 4]\n",
    "    d_values = range(0, 2)\n",
    "    q_values = range(0, 2)\n",
    "    warnings.filterwarnings(\\ignore\\)\n",
    "    evaluate_models(X.values, p_values, d_values, q_values)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    #Xvalues = confirmed_cases.values\n",
    "    Xvalues = X.values\n",
    "    \n",
    "    \n",
    "    size = int(len(Xvalues) * 0.66)\n",
    "    train, test = Xvalues[0:size], Xvalues[size:len(Xvalues)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "    \\tmodel = ARIMA(history, order=(0,1,1))\n",
    "    \\tmodel_fit = model.fit(disp=0)\n",
    "    \\toutput = model_fit.forecast()\n",
    "    \\tyhat = output[0]\n",
    "    \\tpredictions.append(yhat)\n",
    "    \\tobs = test[t]\n",
    "    \\thistory.append(obs)\n",
    "    \\tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    print('Test MSE: %.3f' % error)\n",
    "    # plot\n",
    "    pyplot.plot(test)\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # AR example\n",
    "    from statsmodels.tsa.ar_model import AR\n",
    "    from random import random\n",
    "    # contrived dataset\n",
    "    data = X.values\n",
    "    size = int(len(data) * 0.66)\n",
    "    train, test = data[0:size], data[size:len(data)]\n",
    "    # fit model\n",
    "    AR_model = AR(data)\n",
    "    AR_model_fit = AR_model.fit()\n",
    "    # make prediction\n",
    "    yhat = AR_model_fit.predict(len(data), len(data))\n",
    "    print(yhat)\n",
    "    \n",
    "    #model_fit.predict(1,1)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Future predictions using Linear Regression \n",
    "    print('Model AR future predictions:')\n",
    "    set(zip(future_forcast_dates[23:23+7], np.round(AR_model_fit.predict(len(data),len(data)+6)) ))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    plt.figure(figsize =(10,5))\n",
    "    plt.plot(future_forcast_dates[23:23+7], model_fit.predict(len(data),len(data)+6) )\n",
    "    plt.show()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: markdown,\n",
    "   metadata: {},\n",
    "   source: [\n",
    "    # AR model\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # MA example\n",
    "    from statsmodels.tsa.arima_model import ARMA\n",
    "    from random import random\n",
    "    # contrived dataset\n",
    "    # fit model\n",
    "    MA_model = ARMA(data, order=(0, 1))\n",
    "    MA_model_fit = MA_model.fit(disp=False)\n",
    "    # make prediction\n",
    "    yhat = MA_model_fit.predict(len(data), len(data))\n",
    "    print(yhat)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Future predictions using Linear Regression \n",
    "    print('Model MA future predictions:')\n",
    "    set(zip(future_forcast_dates[23:23+7], np.round(MA_model_fit.predict(len(data),len(data)+6)) ))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: markdown,\n",
    "   metadata: {},\n",
    "   source: [\n",
    "    # Autoregressive Moving Average (ARMA)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # ARMA example\n",
    "    from statsmodels.tsa.arima_model import ARMA\n",
    "    from random import random\n",
    "    # contrived dataset\n",
    "    # fit model\n",
    "    ARMA_model = ARMA(data, order=(2, 1))\n",
    "    ARMA_model_fit = ARMA_model.fit(disp=False)\n",
    "    # make prediction\n",
    "    yhat = ARMA_model_fit.predict(len(data), len(data))\n",
    "    print(yhat)\n",
    "    \\n\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Future predictions using Linear Regression \n",
    "    print('Model ARMA future predictions:')\n",
    "    set(zip(future_forcast_dates[23:23+7], np.round(ARMA_model_fit.predict(len(data),len(data)+6)) ))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: markdown,\n",
    "   metadata: {},\n",
    "   source: [\n",
    "    # Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # SARIMA example\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from random import random\n",
    "    # contrived dataset\n",
    "    #data = [x + random() for x in range(1, 100)]\n",
    "    # fit model\n",
    "    SARIMAX_model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 1))\n",
    "    SARIMAX_model_fit = SARIMAX_model.fit(disp=False)\n",
    "    # make prediction\n",
    "    yhat = SARIMAX_model_fit.predict(len(data), len(data))\n",
    "    print(yhat)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Future predictions using Linear Regression \n",
    "    print('Model SARIMAX future predictions:')\n",
    "    set(zip(future_forcast_dates[23:23+7], np.round(SARIMAX_model_fit.predict(len(data),len(data)+6)) ))\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    ## Vector Autoregression (VAR)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # VAR example\n",
    "    from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "    from random import random\n",
    "    # contrived dataset with dependency\n",
    "    data = pd.concat([confirmed_cases, total], axis=1)\n",
    "    # fit model\n",
    "    model = VAR(data)\n",
    "    model_fit = model.fit()\n",
    "    # make prediction\n",
    "    yhat = model_fit.forecast(model_fit.y, steps=1)\n",
    "    print(yhat)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    import statsmodels.api as sm\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # This is the class definition. Object oriented programming has the concept\n",
    "    # of inheritance, whereby classes may be \\children\\ of other classes. The\n",
    "    # parent class is specified in the parentheses. When defining a class with\n",
    "    # no parent, the base class `object` is specified instead.\n",
    "    class Point(object):\n",
    "    \n",
    "        # The __init__ function is a special method that is run whenever an\n",
    "        # object is created. In this case, the initial coordinates are set to\n",
    "        # the origin. `self` is a variable which refers to the object instance\n",
    "        # itself.\n",
    "        def __init__(self):\n",
    "            self.x = 0\n",
    "            self.y = 0\n",
    "    \n",
    "        def change_x(self, dx):\n",
    "            self.x = self.x + dx\n",
    "    \n",
    "        def change_y(self, dy):\n",
    "            self.y = self.y + dy\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "     #An object of class Point is created\n",
    "    point_object = Point()\n",
    "    \n",
    "    # The object exposes it's attributes\n",
    "    print(point_object.x)  # 0\n",
    "    \n",
    "    # And we can call the object's methods\n",
    "    # Notice that although `self` is the first argument of the class method,\n",
    "    # it is automatically populated, and we need only specify the other\n",
    "    # argument, `dx`.\n",
    "    point_object.change_x(-2)\n",
    "    print(point_object.x)  # -2\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # This is the new class definition. Here, the parent class, `Point`, is in\n",
    "    # the parentheses.\n",
    "    class Vector(Point):\n",
    "    \n",
    "        def __init__(self, x, y):\n",
    "            # Call the `Point.__init__` method to initialize the coordinates\n",
    "            # to the origin\n",
    "            super(Vector, self).__init__()\n",
    "    \n",
    "            # Now change to coordinates to those provided as arguments, using\n",
    "            # the methods defined in the parent class.\n",
    "            self.change_x(x)\n",
    "            self.change_y(y)\n",
    "    \n",
    "        def length(self):\n",
    "            # Notice that in Python the exponentiation operator is a double\n",
    "            # asterisk, \\**\\\n",
    "            return (self.x**2 + self.y**2)**0.5\n",
    "    \n",
    "    # An object of class Vector is created\n",
    "    vector_object = Vector(1, 1)\n",
    "    print(vector_object.length())  # 1.41421356237\\n\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Create a new class with parent sm.tsa.statespace.MLEModel\n",
    "    class LocalLevel(sm.tsa.statespace.MLEModel):\n",
    "    \n",
    "        # Define the initial parameter vector; see update() below for a note\n",
    "        # on the required order of parameter values in the vector\n",
    "        start_params = [1.0, 1.0]\n",
    "    \n",
    "        # Recall that the constructor (the __init__ method) is\n",
    "        # always evaluated at the point of object instantiation\n",
    "        # Here we require a single instantiation argument, the\n",
    "        # observed dataset, called `endog` here.\n",
    "        def __init__(self, endog):\n",
    "            super(LocalLevel, self).__init__(endog, k_states=1)\n",
    "    \n",
    "            # Specify the fixed elements of the state space matrices\n",
    "            self['design', 0, 0] = 1.0\n",
    "            self['transition', 0, 0] = 1.0\n",
    "            self['selection', 0, 0] = 1.0\n",
    "    \n",
    "            # Initialize as approximate diffuse, and \\burn\\ the first\n",
    "            # loglikelihood value\n",
    "            self.initialize_approximate_diffuse()\n",
    "            self.loglikelihood_burn = 1\n",
    "    \n",
    "        # Here we define how to update the state space matrices with the\n",
    "        # parameters. Note that we must include the **kwargs argument\n",
    "        def update(self, params, **kwargs):\n",
    "            # Using the parameters in a specific order in the update method\n",
    "            # implicitly defines the required order of parameters\n",
    "            self['obs_cov', 0, 0] = params[0]\n",
    "            self['state_cov', 0, 0] = params[1]\n",
    "    \n",
    "    # Instantiate a new object\n",
    "    nile_model_1 = LocalLevel(X)\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    nile_model_1\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Compute the loglikelihood at values specific to the nile model\n",
    "    print(nile_model_1.loglike([15099.0, 1469.1]))  # -632.537695048\n",
    "    \n",
    "    # Try computing the loglikelihood with a different set of values; notice that it is different\n",
    "    print(nile_model_1.loglike([10000.0, 1.0]))  # -687.5456216\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Retrieve filtering output\n",
    "    nile_filtered_1 = nile_model_1.filter([15099.0, 1469.1])\n",
    "    # print the filtered estimate of the unobserved level\n",
    "    print(nile_filtered_1.filtered_state[0])         # [ 1103.34065938  ... 798.37029261 ]\n",
    "    print(nile_filtered_1.filtered_state_cov[0, 0])  # [ 14874.41126432  ... 4032.15794181 ]\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Retrieve smoothing output\n",
    "    nile_smoothed_1 = nile_model_1.smooth([15099.0, 1469.1])\n",
    "    # print the smoothed estimate of the unobserved level\n",
    "    print(nile_smoothed_1.smoothed_state[0])         # [ 1107.20389814 ... 798.37029261 ]\n",
    "    print(nile_smoothed_1.smoothed_state_cov[0, 0])  # [ 4015.96493689  ... 4032.15794181 ]\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # Retrieve a simulation smoothing object\n",
    "    nile_simsmoother_1 = nile_model_1.simulation_smoother()\n",
    "    \n",
    "    # Perform first set of simulation smoothing recursions\n",
    "    nile_simsmoother_1.simulate()\n",
    "    print(nile_simsmoother_1.simulated_state[0, :-1])  # [ 1000.09720165 ... 882.30604412 ]\n",
    "    \n",
    "    # Perform second set of simulation smoothing recursions\n",
    "    nile_simsmoother_1.simulate()\n",
    "    print(nile_simsmoother_1.simulated_state[0, :-1])  # [ 1153.62271051 ... 808.43895425 ]\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    # BEFORE: Perform some simulations with the original parameters\n",
    "    nile_simsmoother_1 = nile_model_1.simulation_smoother()\n",
    "    nile_model_1.update([15099.0, 1469.1])\n",
    "    nile_simsmoother_1.simulate()\n",
    "    # ...\n",
    "    \n",
    "    # AFTER: Perform some new simulations with new parameters\n",
    "    nile_model_1.update([10000.0, 1.0])\n",
    "    nile_simsmoother_1.simulate()\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    class ARMA11(sm.tsa.statespace.MLEModel):\n",
    "    \n",
    "        start_params = [0, 0, 1]\n",
    "    \n",
    "        def __init__(self, endog):\n",
    "            super(ARMA11, self).__init__(\n",
    "                X, k_states=2, k_posdef=1, initialization='stationary')\n",
    "    \n",
    "            self['design', 0, 0] = 1.\n",
    "            self['transition', 1, 0] = 1.\n",
    "            self['selection', 0, 0] = 1.\n",
    "    \n",
    "        def update(self, params, **kwargs):\n",
    "            self['design', 0, 1] = params[1]\n",
    "            self['transition', 0, 0] = params[0]\n",
    "            self['state_cov', 0, 0] = params[2]\n",
    "    \n",
    "    # Example of instantiating a new object, updating the parameters to the\n",
    "    # starting parameters, and evaluating the loglikelihood\n",
    "    inf_model = ARMA11(X)\n",
    "    print(inf_model.loglike(inf_model.start_params))  # -2682.72563702\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: [\n",
    "    class SimpleRBC(sm.tsa.statespace.MLEModel):\n",
    "    \n",
    "        start_params = [...]\n",
    "    \n",
    "        def __init__(self, endog):\n",
    "            super(SimpleRBC, self).__init__(\n",
    "                endog, k_states=2, k_posdef=1, initialization='stationary')\n",
    "    \n",
    "            # Initialize RBC-specific variables, parameters, etc.\n",
    "            # ...\n",
    "    \n",
    "            # Setup fixed elements of the statespace matrices\n",
    "    self['selection', 1, 0] = 1\n",
    "    \n",
    "                def solve(self, structural_params):\n",
    "                    # Solve the RBC model\n",
    "                    # ...\n",
    "    \n",
    "                    def update(self, params, **kwargs):\n",
    "                        params = super(SimpleRBC, self).update(params, **kwargs)\n",
    "    \n",
    "    # Reconstruct the full parameter vector from the\n",
    "     # estimated and calibrated parameters\n",
    "    structural_params = ...\n",
    "        measurement_variances = ...\n",
    "    \n",
    "        # Solve the model\n",
    "        design, transition = self.solve(structural_params)\n",
    "    \n",
    "        # Update the statespace representation\n",
    "        self['design'] = design\n",
    "        self['obs_cov', 0, 0] = measurement_variances[0]\n",
    "        self['obs_cov', 1, 1] = measurement_variances[1]\n",
    "        self['obs_cov', 2, 2] = measurement_variances[2]\n",
    "        self['transition'] = transition\n",
    "        self['state_cov', 0, 0] = structural_params[...]\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   cell_type: code,\n",
    "   execution_count: null,\n",
    "   metadata: {},\n",
    "   outputs: [],\n",
    "   source: []\n",
    "  }\n",
    " ],\n",
    " metadata: {\n",
    "  kernelspec: {\n",
    "   display_name: Python 3,\n",
    "   language: python,\n",
    "   name: python3\n",
    "  },\n",
    "  language_info: {\n",
    "   codemirror_mode: {\n",
    "    name: ipython,\n",
    "    version: 3\n",
    "   },\n",
    "   file_extension: .py,\n",
    "   mimetype: text/x-python,\n",
    "   name: python,\n",
    "   nbconvert_exporter: python,\n",
    "   pygments_lexer: ipython3,\n",
    "   version: 3.7.6\n",
    "  }\n",
    " },\n",
    " nbformat: 4,\n",
    " nbformat_minor: 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
